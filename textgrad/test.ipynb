{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textgrad as tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg.set_backward_engine(\"gpt-4o\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get an initial response from an LLM.\n",
    "model = tg.BlackboxLLM(\"gpt-3.5-turbo\")\n",
    "question_string = (\"If it takes 1 hour to dry 25 shirts under the sun, \"\n",
    "                   \"how long will it take to dry 30 shirts under the sun? \"\n",
    "                   \"Let's think step by step in order to produce the answer.\")\n",
    "\n",
    "question = tg.Variable(question_string, \n",
    "                       role_description=\"question to the LLM\", \n",
    "                       requires_grad=False)\n",
    "\n",
    "answer = model(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer.set_role_description(\"concise and accurate answer to the question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the loss function and the optimizer, just like in PyTorch! \n",
    "# Here, we don't have SGD, but we have TGD (Textual Gradient Descent) \n",
    "# that works with \"textual gradients\". \n",
    "optimizer = tg.TGD(parameters=[answer])\n",
    "evaluation_instruction = (f\"Here's a question: {question_string}. \" \n",
    "                           \"Evaluate any given answer to this question, \"\n",
    "                           \"be smart, logical, and very critical. \"\n",
    "                           \"Just provide concise feedback.\")\n",
    "                            \n",
    "\n",
    "# TextLoss is a natural-language specified loss function that describes \n",
    "# how we want to evaluate the reasoning.\n",
    "loss_fn = tg.TextLoss(evaluation_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It will take 1.2 hours to dry 30 shirts under the sun. Since 25 shirts dry in 1 hour, the rate is 25 shirts per hour. Therefore, drying 30 shirts will take 1.2 hours (30/25). This assumes constant drying conditions.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Do the loss computation, backward pass, and update the punchline. \n",
    "# Exact same syntax as PyTorch!\n",
    "loss = loss_fn(answer)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable(value=It will take 1.2 hours to dry 30 shirts under the sun. Since 25 shirts dry in 1 hour, the rate is 25 shirts per hour. Therefore, drying 30 shirts will take 1.2 hours (30/25). This assumes constant drying conditions., role=concise and accurate answer to the question, grads=Here is a conversation:\n",
       "\n",
       "<CONVERSATION><LM_SYSTEM_PROMPT> Here's a question: If it takes 1 hour to dry 25 shirts under the sun, how long will it take to dry 30 shirts under the sun? Let's think step by step in order to produce the answer.. Evaluate any given answer to this question, be smart, logical, and very critical. Just provide concise feedback. </LM_SYSTEM_PROMPT>\n",
       "\n",
       "<LM_INPUT> Sure, let's break it down step by step:\n",
       "\n",
       "Step 1: Calculate the rate at which shirts are drying\n",
       "If it takes 1 hour to dry 25 shirts, then the rate at which shirts are drying is 25 shirts per hour.\n",
       "\n",
       "Step 2: Calculate the time it takes to dry 1 shirt\n",
       "To find out how long it takes to dry 1 shirt, we divide the total time by the number of shirts:\n",
       "1 hour / 25 shirts = 0.04 hours per shirt\n",
       "\n",
       "Step 3: Calculate the time it takes to dry 30 shirts\n",
       "Now that we know it takes 0.04 hours to dry 1 shirt, we can calculate how long it will take to dry 30 shirts:\n",
       "0.04 hours per shirt x 30 shirts = 1.2 hours\n",
       "\n",
       "Therefore, it will take 1.2 hours to dry 30 shirts under the sun. </LM_INPUT>\n",
       "\n",
       "<LM_OUTPUT> Your step-by-step breakdown is logical and mathematically sound. However, the initial assumption that drying 30 shirts will take proportionally longer than drying 25 shirts may not always hold true in real-world scenarios. Factors such as the drying area, sunlight intensity, and shirt arrangement could affect the drying time. But based on the given information and assuming all conditions remain constant, your calculation is correct. It will take 1.2 hours to dry 30 shirts under the sun. </LM_OUTPUT>\n",
       "\n",
       "</CONVERSATION>\n",
       "\n",
       "This conversation is potentially part of a larger system. The output is used as response from the language model\n",
       "\n",
       "Here is the feedback we got for concise and accurate answer to the question in the conversation:\n",
       "\n",
       "<FEEDBACK>1. **Conciseness**: The step-by-step breakdown, while thorough, could be more concise. For a concise and accurate answer, consider summarizing the steps without explicitly numbering them. This would make the response shorter and more direct.\n",
       "\n",
       "2. **Assumptions**: The answer should briefly acknowledge the assumption that drying conditions remain constant. This would address the potential real-world variability mentioned in the evaluation.\n",
       "\n",
       "3. **Clarity**: Ensure that the mathematical operations are clear and easy to follow. While the current explanation is clear, it could be slightly streamlined for better readability.\n",
       "\n",
       "4. **Direct Answer**: Start with the direct answer before providing the breakdown. This approach ensures that the reader gets the answer immediately and can then understand the reasoning if they choose to read further.\n",
       "\n",
       "5. **Redundancy**: Remove any redundant phrases. For example, \"Sure, let's break it down step by step\" can be omitted to make the response more concise.\n",
       "\n",
       "### Specific Feedback:\n",
       "- **Initial Statement**: Remove \"Sure, let's break it down step by step\" to make the response more concise.\n",
       "- **Step 1**: Instead of \"Calculate the rate at which shirts are drying,\" simply state, \"The drying rate is 25 shirts per hour.\"\n",
       "- **Step 2**: Combine the calculation of drying one shirt and 30 shirts into a single step for brevity.\n",
       "- **Assumptions**: Add a brief statement acknowledging the assumption of constant drying conditions.\n",
       "\n",
       "### Example of Streamlined Feedback:\n",
       "- Start with the direct answer: \"It will take 1.2 hours to dry 30 shirts under the sun.\"\n",
       "- Follow with a concise explanation: \"Since 25 shirts dry in 1 hour, the rate is 25 shirts per hour. Therefore, drying 30 shirts will take 1.2 hours (30/25). This assumes constant drying conditions.\"\n",
       "\n",
       "By implementing these changes, the answer will be more concise and still accurate, improving the objective function.</FEEDBACK>\n",
       "\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
