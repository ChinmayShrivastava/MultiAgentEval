import asyncio
import re
import uuid

import mlflow
import tqdm
from llama_index.llms.openai import OpenAI
from prompts import DEFAULT_ANSWER_PROMPT
from pydantic import BaseModel
import dspy 
from openai import OpenAI
import os

DEFAULT_MODEL = 'gpt-4o'
BATCH_SIZE = 1

client = OpenAI(
    api_key=os.environ.get("MARTIAN_API_KEY"),  # defaults to os.environ.get("OPENAI_API_KEY")
    base_url="https://withmartian.com/api/openai/v1",
)

class MMLU(BaseModel):
    question: str
    answers: list[str]
    correct: str

class DUPagent:
    def __init__(
        self,
        # llm: OpenAI = None,
    ):
        # self.llm = llm or OpenAI(model=DEFAULT_MODEL)

        self._question = None

        # # set up mlflow
        # mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
        # mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)

    @property
    def question(self):
        return self._question
    
    @question.setter
    def question(self, question: MMLU):
        self._question = question
    
    # async def generate_hints(self, question: MMLU) -> str:
    #     _prompt = GENERATE_HINTS.format(question=question.question)
    #     res = await self.llm.acomplete(_prompt)
    #     return res.text
    
    async def generate_answer(
        self,
        question: str,
        answers: list[str],
    ) -> str:
        _answers = ""
        for i, answer in enumerate(answers):
            _answers += f"{chr(65+i)}. {answer}\n"
        _prompt = DEFAULT_ANSWER_PROMPT.format(question=question, options=_answers)
        # res = await self.llm.acomplete(_prompt)
        res = client.chat.completions.create(
            model="router",
            messages=[
                {
                    "role": "system",
                    "content": _prompt,
                }
            ],
        )
        reason, answer = self.parse_response(res.choices[0].message.content)
        return reason, answer

    # async def generate_answer_majority_vote(
    #     self,
    #     question: str,
    #     answers: list[str],
    #     hints: str,
    # ) -> str:
    #     _answers = ""
    #     for i, answer in enumerate(answers):
    #         _answers += f"{chr(65+i)}. {answer}\n"
    #     _prompt = DUP_GENERATE_ANSWER.format(question=question, options=_answers, hints=hints)
    #     completions = []
    #     for _ in range(5):
    #         res = await self.llm.acomplete(_prompt)
    #         reason, answer = self.parse_response(res.text)
    #         data = {
    #             "rationale": reason,
    #             "answer": answer
    #         }
    #         completions.append(data)
    #     ensemble = dspy.majority(dspy.Completions(completions))

    #     return ensemble.rationale, ensemble.answer

    def parse_response(self, response: str) -> str:
        try:
            r = re.search(r"(?s)REASONING:\s*(.*?)ANSWER", response).group(1)
        except:
            r = ""
        try:
            a = re.search(r"ANSWER:\s*([A-E])", response).group(1)
        except:
            _a = client.chat.completions.create(
                model="router",
                messages=[
                    {
                        "role": "system",
                        "content": f"""Return the answer that was generated by the model. Return the alphabet letter corresponding to the answer.
    Generated Response:
    {response}
    Alphabet for the correct Answer:""",
                    }
                ],
            ).choices[0].message.content
            if "A" in _a.upper():
                a = "A"
            elif "B" in _a.upper():
                a = "B"
            elif "C" in _a.upper():
                a = "C"
            elif "D" in _a.upper():
                a = "D"
            else:
                a = ""
        return r, a
    
    async def get_answer(self, question: MMLU) -> str:
        # hints = await self.generate_hints(question)
        reason, answer = await self.generate_answer(question.question, question.answers)
        return reason, answer, ""
    
    async def generate_answers(self, questions: list[MMLU], batch=BATCH_SIZE) -> tuple[list[str], list[str]]:
        answers = []
        reasons = []
        hints = []
        for i in tqdm.tqdm(range(0, len(questions), batch), desc="Generating answers", leave=False):
            batch_questions = questions[i:i+batch]
            batch_answers = await asyncio.gather(*[self.get_answer(question) for question in batch_questions])
            for reason, answer, hint in batch_answers:
                reasons.append(reason)
                answers.append(answer)
                hints.append(hint)
        return reasons, answers, hints
    
# def run_eval(
# 		agent: DUPagent,
# 		questions: list[MMLU],
# ) -> list[str]:
# 	import tqdm
# 	responses = []
# 	answers = []
# 	for question in tqdm.tqdm(questions):
# 		agent.reset()
# 		agent.question = question
# 		res, answer = agent.get_response()
# 		responses.append(res)
# 		answers.append(answer)
# 	return responses, answers

async def arun_eval(
		questions: list[MMLU]
) -> list[str]:
    answers = []
    reasons = []
    hints = []
    agent = DUPagent()
    res = await agent.generate_answers(questions)
    reasons.extend(res[0])
    answers.extend(res[1])
    hints.extend(res[2])
    return reasons, answers, hints
    

if __name__ == "__main__":
    agent = DUPagent()
    questions = [MMLU(
		question="What is the capital of France?",
		answers=["Paris", "London", "Berlin", "Madrid"],
		correct="A"
	),
    MMLU(
        question="What is the capital of Germany?",
        answers=["Paris", "London", "Berlin", "Madrid"],
        correct="C"
    )]
    print(asyncio.run(arun_eval(questions)))